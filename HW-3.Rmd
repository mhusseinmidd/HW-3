---
title: "MATH 216 Homework 3"
author: "Mohamed Hussein"
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(plyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(Quandl))
suppressPackageStartupMessages(library(plotly))
suppressPackageStartupMessages(library(pander))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(ROCR))
```


## Admistrative:

Please indicate

* Who you collaborated with: None
* Roughly how much time you spent on this HW: 5 Hours
* What gave you the most trouble: Question 1
* Any comments you have: Question 1 is in a very crude state as it currently stand. 
Would love to discuss the concepts related to machine learning in class. In 
addition, what are the statistical implications for losing a considerable chunk 
of the dataset as a result of the variables chosen and how thresholds are chosen. 


## Data

* You must first copy the file `profiles.csv` from `HW-2` to the `data` folder
in the `HW-3` directory
* We also consider all 222,540 songs played in the Reed College pool hall
jukebox from Nov 30, 2003 to Jan 22, 2009 (included in `HW-3` folder). 

```{r, echo=FALSE, cache=TRUE}
# DO NOT EDIT THIS SECTION!
profiles <- read.csv("data/profiles.csv", header=TRUE) %>% 
  tbl_df()

essays <- select(profiles, contains("essay"))
profiles <- select(profiles, -contains("essay"))

jukebox <- read.csv("data/jukebox.csv", header=TRUE) %>% 
  tbl_df()

bitcoin <-  Quandl("BAVERAGE/USD") %>% tbl_df()

gold<- Quandl("WGC/GOLD_DAILY_USD", start_date="2010-07-17") %>% 
  tbl_df()
```


## Question 1:

For this question we will be picking up from where we left off in HW-2,
specifically the OkCupid dataset.


### a)

Using your exploratory data analysis from HW-2, fit a logistic regression to
predict individual's gender and interpret your results.

```{r, echo=FALSE, warning=FALSE, fig.width=12, fig.height=6}
#Prep variables: 
profiles <- 
  profiles %>% 
  filter (income!=-1) %>%
  filter(body_type!="rather not say") %>% 
  filter(body_type!="") %>%  
  filter (smokes!="") %>%
  filter (drugs!="") %>% 
  filter (drinks!="") %>% 
  mutate (is_female = ifelse(sex=="f", 1, 0), 
          log_income=log(income), 
          education=
            revalue(education, c("dropped out of law school"=
                                   "Bachelor's",
                                 "dropped out of masters program"=
                                   "Bachelor's", 
                                 "dropped out of med school"=
                                   "Bachelor's",
                                 "dropped out of ph.d program"=
                                   "Bachelor's", 
                                 "graduated from college/university"=
                                   "Bachelor's", 
                                 "college/university"=
                                   "Bachelor's", 
                                 "graduated from law school"=
                                   "Graduate or Professional Degree", 
                                 "graduated from masters program"=
                                   "Graduate or Professional Degree",
                                 "graduated from med school"=
                                   "Graduate or Professional Degree", 
                                 "graduated from ph.d program"=
                                   "Graduate or Professional Degree", 
                                 "law school"=
                                   "Graduate or Professional Degree", 
                                 "masters program" =
                                   "Graduate or Professional Degree", 
                                 "med school" = 
                                   "Graduate or Professional Degree", 
                                 "ph.d program"= 
                                   "Graduate or Professional Degree", 
                                 "working on masters program" = 
                                   "Graduate or Professional Degree",
                                 "working on ph.d program" = 
                                   "Graduate or Professional Degree",
                                 "working on med school" = 
                                   "Graduate or Professional Degree",
                                 "working on law school" = 
                                   "Graduate or Professional Degree",
                                 "working on college/university"= 
                                   "Some College", 
                                 "graduated from two-year college"=
                                   "Some College", 
                                 "dropped out of college/university"=
                                   "Some College", 
                                 "working on two-year college"=
                                   "Some College", 
                                 "two-year college" = 
                                   "Some College", 
                                 "dropped out of two-year college"=
                                   "Some College", 
                                 "graduated from high school"=
                                   "High School", 
                                 "high school"=
                                   "High School", 
                                 "dropped out of high school"=
                                   "High School", 
                                 "working on high school"=
                                   "High School",
                                 "graduated from space camp"=
                                   "Space Camp", 
                                 "working on space camp"=
                                   "Space Camp", 
                                 "dropped out of space camp"=
                                   "Space Camp", 
                                 "space camp"=
                                   "Space Camp")
                    ), 
          status=
            revalue(status, c("seeing someone"=
                                "In a Relationship",
                              "married"=
                                "In a Relationship", 
                              "available"=
                                "single")
                    ), 
          smokes=
          revalue(smokes, c("sometimes"= "Yes","trying to quit"= "Yes", 
                            "when drinking"= "Yes", "yes"= "Yes", "no"="No"
                            )
                  ), 
          drugs=
          revalue(drugs, c("never"="No", "often"="Yes",
                           "sometimes"="Yes"
                           )
                  ),
          drinks=
          revalue(drinks, c("not at all"="No", "rarely"="No", 
                            "desperately"="Yes", "often"="Yes", 
                            "socially"="Yes", "very often"="Yes"
                            )
                  )
          )
#Now that our variables are cleaned, we create our regression model. We have 
# multi-levelled categorical variables, so we use the model matrix option: 
# model1 <- model.matrix(sex~age+
#                          height+
#                          log_income+
#                          education+
#                          body_type+
#                          status+
#                          smokes+
#                          drugs+
#                          drinks, 
#                        data=profiles, family=binomial)
# 
# model1 <- data.frame(model1)
#This is NOT working, so we use brute force: 

profiles <- 
  profiles %>%  
  mutate (smc=ifelse(education=="Some College", 1,0), 
          grad=ifelse(education=="Graduate or Professional Degree", 1,0),
          spc=ifelse(education=="Space Camp", 1,0),
          ath=ifelse(body_type=="athletic", 1,0), 
          cur=ifelse(body_type=="curvy", 1,0), 
          fit=ifelse(body_type=="fit", 1,0),
          ff=ifelse(body_type=="full figured", 1,0), 
          smokes=ifelse(smokes=="Yes", 1,0), 
          drinks=ifelse(drinks=="Yes", 1,0), 
          drugs=ifelse(drugs=="Yes", 1,0), 
          is_female=ifelse(sex=="f", 1,0)
  ) %>% 
  select (is_female, age, smc, grad, spc, ath, cur, fit, ff, smokes, drinks, 
          drugs, age, height, log_income)

logistic_model <- 
  glm(is_female ~ age + smc + grad + spc + ath + cur + fit + ff + smokes 
                      + drinks + drugs + height + log_income, family=binomial(), profiles)

panderOptions("digits", 3)
pander (logistic_model, covariate.labels =c("Age", 'Some College', 
                                            'Graduate/ Professional Degree',
                                            'Space Camp', 'Atheltic', 'Curvey', 
                                            'Fit', 'Full Figured', 'Smokes',
                                            'Drinks', 'Does Drugs', 'Height', 
                                            'Logged Income'))
```

The results of our regression model show that age is a statistically significant predictor of sex. A one year increase in age, on average, is associated with a 1.021 multiplicative increase in the odds of being female, cetris paribus. This age logged odds estimate (0.0212) lies witin a 95% confidence interval that streches from 0.0142	to 0.0281. Conversely, having only some college education, compared to having a Bachelor's, is associated with a decrease in the odds of being female by 0.802 and its logged odds ratio falls within a confidence interval of [-0.396,	-0.0438]. Surprisingly, smoking does not look signififcant, while drinking is associated with an increase in the odds of being female (!)

```{r, echo=FALSE, warning=FALSE, fig.width=12, fig.height=6}
ci_logistic_model <-confint.default(logistic_model)
row.names(ci_logistic_model)<-c("Intercept", "Age", 
                              'Some College', 
                              'Graduate/ Professional Degree',
                              'Space Camp', 'Atheltic', 'Curvey', 
                              'Fit', 'Full Figured', 'Smokes',
                              'Drinks', 'Does Drugs', 'Height', 
                              'Logged Income')
pander(ci_logistic_model)
```

### b)

Plot a histogram of the fitted probabilities $\widehat{p}_i$ for all users $i=1,
\ldots, n=59946$ in your dataset.

```{r, echo=FALSE, fig.width=12, fig.height=6, warning=FALSE}
profiles <- profiles %>% 
            mutate (fitted_probability=fitted(logistic_model))

hist(fitted(logistic_model))
```
This histogram shows the distribution of the fitted probabilitys $\widehat{p}_i$ for users in the okcupid dataset. Given the model specififcation chosen above, notice that the total number of observation drops from 59,946 to 8,879. 
### c)

Use a *decision threshold* of $p^*=0.5$ to make an explicit prediction for each
user $i$'s sex and save this in a variable `predicted_sex`. In other words, for user $i$

* If $\widehat{p}_i > p^*$, set `predicted_sex = 1` i.e. they are female
* If $\widehat{p}_i < p^*$, set `predicted_sex = 0` i.e. they are male

Display a 2 x 2 contigency table of `sex` and `predicted_sex` i.e. compare the 
predicted sex to the actual sex of all users. The sum of all the elements in
your table should be $n=59946$. Comment on how well our predictions fared.

The contingency table below displays predicted and actual sexes of Okcupid users in the Bay Area. Overall, our model seems to have fared well. Of the 6,742  males in our sample, we were able to predict accuratley the sex of 6,043. That is, our model correctly predicted the gender of males 90% of the time. As for females, the model predicted correctly their sex 83.7% of the time. Combined, our model made correct predictions 88.3% of the time regarding an Okcupid user's sex. As mentioned above, the total number of users is less than the expected $n=59946$, since regression models drop users for whom no information on a certain variable is available. 
```{r, echo=FALSE, fig.width=12, fig.height=6}
profiles <- profiles %>% 
  mutate (predicted_sex_logodds=predict(logistic_model), 
          
          #Find the odds: 
          
          predicted_sex_odds=exp(predicted_sex_logodds), 
          
          #Now, convert to probability: 
          
          predicted_sex_prob=predicted_sex_odds/(1+predicted_sex_odds), 
          
          #We have the probability that someone is female based on our model. Now,            we incorporate the threshold: 
          
          predicted_sex=ifelse(predicted_sex_prob>=0.5, 1, 0)
          )

profiles$is_female <- factor(profiles$is_female, levels = c(0, 1),
labels = c("Male", "Female"))

colnames <- c("Predicted Male", "Predicted Female")
#We can compare actual sex to predicted sex: 
compare <- table(profiles$is_female, profiles$predicted_sex)
kable (compare, col.names=colnames
       )
```


### d)

Say we wanted to have a **false positive rate** of about 20%, i.e. of the people
we predicted to be female, we want to be wrong no more than 20% of the time. What
decision threshold $p^*$ should we use?

False positive refers to instances of falsely rejecting the null hypothesis. In this case, it refers to predicting that a user is female when in fact they are male. As it currently stands, the model has a false positive rate of 17.3% only. In any case, to determine the best theshold for our model, we draw a Receiver Operating Characteristic Curve (ROC). 
```{r, echo=FALSE, fig.width=12, fig.height=6}
#Create a prediction variable: 
pred<-prediction(profiles$predicted_sex_prob, profiles$predicted_sex)
roc.perf <- performance(pred, measure='tpr', x.measure='fpr')
plot (roc.perf)
abline(a=0, b= 1)
```
Professor, I am unclear on how to interpret this graph. It seems too far away from the diagonal line ($x=y$). What are your thoughts? 

## Question 2:

Using the jukebox data, plot a time series of the number of songs played each
week over the entire time period. i.e.

* On the x-axis present actual dates (not something like Week 93, which doesn't 
mean anything to most people).
* On the y-axis present the total number of songs.

What seasonal (i.e. cyclical) patterns do you observe?

```{r, echo=FALSE, fig.width=12, fig.height=6}
jukebox <- read.csv("data/jukebox.csv", header=TRUE) %>% 
  tbl_df()

time_series <- 
  jukebox %>%  
  mutate (date_fixed=parse_date_time(date_time, '%m %d %h:%m:%s %y'), 
          week=floor_date(date_fixed, "week")
  ) %>% 
  group_by(week) %>% 
  tally()

mean_time_series <-
  time_series %>% 
  summarise(mean_n=mean(n))
  
q2 <- 
  ggplot(time_series, aes(x=week, y=n)) + 
  geom_line() + 
  labs(x="Week of Year", 
      y="Total Number of Played Songs",
      title="Number of Played Songs per Week")+
  geom_hline(yintercept=827, color='red')
ggplotly(q2)
```

This graph displays the total number of songs played on the juke box for every week of the years 2004-2009 at Reed College's pool hall. On average, as the red line shows, 827 songs were played per week. Looking at the graph, it seems that the begining and end of every year witnessesa considerable increase in the total number of songs played per week at Reed's college. This pattern is expected, since there are presuemably fewer students in the summer and during winter break, when the number of songs played is lowest, compared to the academic year which spans from September to May. It is unclear why the number of songs shoots up to 1,641, which is two standard deviations away from th emean of 827, in the week of 27 of January 2007. 

## Question 3:

Using the jukebox data, what are the top 10 artists played during the "graveyard
shift" during the academic year? Define

* the "graveyard shift" as midnight to 8am
* the academic year as September through May (inclusive)

```{r, echo=FALSE, fig.width=12, fig.height=6}
graveyard <- 
  jukebox %>% 
  mutate (date_fixed=parse_date_time(date_time, '%m %d %h:%m:%s %y')) %>% 
  filter(month(date_fixed)<=05|month(date_fixed)>=09) %>%
  filter (hour(date_fixed)>=00 & hour(date_fixed)<=08) %>%  
  group_by(artist) %>% 
  tally() %>%
  arrange (desc(n)) %>% 
  slice (1:10)

pander(graveyard)
```
The top artist played at Reed college's pool hall on the jukebox between 2004 and 2009 during the graveyard shift in the academic year is OutKast. The hip-hop duo beats its closest contester The Beatles by a considerable margin of 399 The Beatles, in turn, was played 643 times more than Led Zeppelin, who came in third place, overtaking Radiohead by merely 81 times. The Talking Heads, who took the tenth place, can perhaps learn a lesson or two from OutKast, who were played 1604 times more than this 70s and 80s rock band.  


Let's look at what the graveyard shift looks like during the summer for comparison purposes: 

```{r, echo=FALSE, fig.width=12, fig.height=6}
graveyard <- 
  jukebox %>% 
  mutate (date_fixed=parse_date_time(date_time, '%m %d %h:%m:%s %y')) %>% 
  filter(month(date_fixed)>=05&month(date_fixed)<=09) %>%
  filter (hour(date_fixed)>=00 & hour(date_fixed)<=08) %>%  
  group_by(artist) %>% 
  tally() %>%
  arrange (desc(n)) %>% 
  slice (1:10)

pander(graveyard)
```

We see that the pattern is largely the same, with a few shifts around. The top artist remains OutKast, although it is played less frequently. The hip-hop duo beats its closest contester The Beatles by a  margin of 149. The Beatles, in turn, were played 132 times more than Radiohead, who switched positions coming in third place and overtaking Led Zeppelin by merely 10 times. Eminem lost ground to THe Red Hot Chili Peppers, Michael Jackson, and the Talking Heads, coming in tenth with  463 plays between the contemporary rapper and OutKast, the overall winner. 


## Question

We want to compare the volatility of 

* bitcoin prices
* gold prices

Let our measure of volatility be the relative change from day-to-day in price. 
Let the reference currency be US dollars. Analyze these results and provide
insight to a foreign currency exchanger.

A visual comparison between the prices of bitcoin and gold relative to the US 
dollar shows that, in the long term, gold is characterized by mild fluctuations. 
More specifically, the amplitude of the volatity of gold is around 417 USD.
Bitcoin, on the other hand, seems to possess a maximum displacement of 600 USD, 
making it more volatile than gold over all. This large difference of almost 200 USD
is driven in part by a drastic spike in the value of Bitcoins in the week of 
29-11-2013, when its media presence was strong, as evident by references in 
articles in CNN Money, Wall Street Journal, and Bloomberg. From Mid-2014 onwards, 
it seems that the fluctuations of Bitcoin have become milder and continue to look 
this way up to the present. 

```{r, echo=FALSE, fig.width=12, fig.height=6}
gold<- Quandl("WGC/GOLD_DAILY_USD", start_date="2010-07-17") %>% 
  tbl_df()

bitcoin_gold <- 
  left_join(bitcoin, gold, by="Date") %>%  
  select (Date, `24h Average`, Value)

q4 <- 
  ggplot(bitcoin_gold, aes(Date)) + 
  geom_line(aes(y = `24h Average`, colour = "Bitcoin")) + 
  geom_line(aes(y = Value, colour = "Gold"))+ 
  labs(x="Year", 
       y="Average Price Per Day", 
       title="Price Volatility of Gold and Bitcoin") 

ggplotly(q4)

```
We can do better by defining volatility as the relative change in price between one day and the previous day. Plotting the volatility for Bitcoin and gold, we get: 

```{r, echo=FALSE, fig.width=12, fig.height=6}
bitcoin_gold <- bitcoin_gold %>% 
  arrange (Date) %>% 
  mutate (bitcoin_volatility = `24h Average` - lag(`24h Average`),
          default=first(`24h Average`), 
          gold_volatility=Value-lag(Value), default=first(Value)
          )
q4_volt<- 
  ggplot(bitcoin_gold, aes(Date)) + 
  geom_line(aes(y = bitcoin_volatility, colour = "Bitcoin")) + 
  geom_line(aes(y = gold_volatility, colour = "Gold"))+ 
  labs(x="Year", 
       y="Price Volatility", 
       title="Price Volatility of Gold and Bitcoin") 

ggplotly(q4_volt)
```

This graph shows Gold to be a consistent currency in its volatility, restricted by a band of about 50% difference in either direction from the day before. Bitcoin, however, seems to have faced much greater volatility, especially in 2014, where the were offshoots of more than 300% in the negative direction and 150% in the positive direction of the percentage change in price from the day before. It does seem that the relative change is converging to a narrower band, from March 2014 onwards. 

Even better would be to rescale the magintude of the difference. 
```{r, echo=FALSE, fig.width=12, fig.height=6}
bitcoin_gold <- 
  bitcoin_gold %>% 
  arrange (Date) %>% 
  mutate (bitcoin_volatility = (`24h Average` - lag(`24h Average`))/lag(`24h Average`),
          default=first(`24h Average`), 
          gold_volatility=(Value-lag(Value))/lag(Value), default=first(Value)
          )
q4_volt_rescaled<- 
  ggplot(bitcoin_gold, aes(Date)) + 
  geom_line(aes(y = bitcoin_volatility, colour = "Bitcoin")) + 
  geom_line(aes(y = gold_volatility, colour = "Gold"))+ 
  labs(x="Year", 
       y="Price Volatility", 
       title="Price Volatility of Gold and Bitcoin") 

ggplotly(q4_volt_rescaled)
```
This graph drives the point home. Gold is condiserably less volatile compared to bitcoin, across the board. It also confirms our earlier suspicion that the volatility of Bitcoin has been decreasing over time. 

## Question 5:

Using the data loaded from Quandl below, plot a time series using `geom_line()`
comparing cheese and milk production in the US from 1930 to today. Comment on this.

* Cheese [page](https://www.quandl.com/data/USDANASS/NASS_CHEESEPRODUCTIONMEASUREDINLB-Cheese-Production-Measured-In-Lb)
* Milk [page](https://www.quandl.com/data/USDANASS/NASS_MILKPRODUCTIONMEASUREDINLB-Milk-Production-Measured-In-Lb)

```{r, echo=FALSE, fig.width=12, fig.height=6, warning=FALSE}
cheese <- Quandl("USDANASS/NASS_CHEESEPRODUCTIONMEASUREDINLB") %>% 
  tbl_df()
milk <-  Quandl("USDANASS/NASS_MILKPRODUCTIONMEASUREDINLB") %>% 
  tbl_df()

milk_cheese <- left_join(cheese, milk, by="Date") 

  ggplot(milk_cheese, aes(Date)) + 
  geom_line(aes(y = Value.x, colour = "Cheese")) + 
  geom_line(aes(y = Value.y, colour = "Milk"))+ 
  labs(x="Year", 
       y="Cheese and Milk Production (LB)", 
       title="Cheese and Milk Production by Year") 
```

It seems that, while Milk production has soared since the 1980s, cheese production has only witnessed a very mild rise in production. The steep rise in milk production may be explained by the demand for milk is driven by more than direct consumer consumption. Cheese, for instance, depends on milk. Hence, the smaller upward trend in cheese may be driving part of the increase in milk production. Afterall, to produce one pound of a cheese like Cheddar, one needs at least 10 pounds of milk (which is a werid metric for measuring milk but guarantees that our axes are the same for two distinct variables).

To make the comparison fairer, let's consider the percentage change in milk and cheese production, relative to the first year we have information on both industries, which is 1924. Effectively, we are creating an index: 
```{r, echo=FALSE, fig.width=12, fig.height=6, warning=FALSE}

milk_cheese <- left_join(cheese, milk, by="Date") 
milk_cheese_change <- 
  milk_cheese %>% 
  arrange (Date) %>% 
  mutate (Value.x = (((Value.x - (Value.x)[6]))/(Value.x)[6]),
          # default=first(Value.x), 
          Value.y = (((Value.y - (Value.y)[6]))/(Value.y)[6])
          )

  ggplot(milk_cheese_change, aes(Date)) + 
  geom_line(aes(y = Value.x, colour = "Cheese")) + 
  geom_line(aes(y = Value.y, colour = "Milk"))+ 
  labs(x="Year", 
       y="Change from 1924 Production", 
       title="Cheese and Milk Production Change by Year") 
```

The trends switch. While before, the naive graph made it seem as if Milk was growing at a faster rate than cheese, it is now evident that cheese is growing quite rapidly. This trend was masked by the sheer size of Milk. It is more difficult to sustain high levels of growth when you are already so big as an industry. Wheras cheese seems to be taking off, especially in the 90s and 2000s.
Note: The y-axis should be understood as how many times is the industry compared to its level in 1924. In case of cheese in 2000, for instance, we would say that production level 15 times larger than production level in 1924. For Milk, it would be 3 times as large. 